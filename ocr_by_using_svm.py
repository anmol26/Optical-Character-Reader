# -*- coding: utf-8 -*-
"""OCR by using SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J5d24EXKjMrJSacR7HiPVFQ65wZY--Z2

#                                                       MINI PROJECT-2 ( OCR using Support Vector Machine )
**Group Members:**

-*Anmol Khandelwal*

-*Aryan Mangla*

-*Devashish Singh*

---



---
"""

# Import the required Libraries
import pandas as pd                            #allows importing data from various file formats like CSV, JSON, SQL
import numpy as np                             #used for working with arrays
from sklearn.svm import SVC                    #Sklearn features various classification, regression & clustering algorithms including SVM     
                                               #SVC (Support Vector Classifier)

import matplotlib.pyplot as plt                #plotting library
import seaborn as sns                          #Seaborn contains a few plots and patterns for data visualisation, while
                                               #in matplotlib,datasets are visualised with the assistance of lines, 
                                               #scatter plots, pie charts, histograms, bar-graphs, etc

from sklearn.model_selection import train_test_split  #for splitting data arrays into two subsets:-
                                                      #for training data and for testing data

from sklearn.metrics import confusion_matrix   #Compute confusion matrix to evaluate the accuracy of a classification
from sklearn import metrics                    #In multilabel classification, this function computes subset accuracy

from sklearn.preprocessing import MinMaxScaler #Transform features by scaling each feature to a given range.B/W 0 & 1 by default 
                                               #preserves the shape of the original distribution

#Read dataset "letter-recognition.csv"
letters = pd.read_csv('letter-recognition.csv')

"""
1. letter capital letter (26 values from A to Z)
2. xbox horizontal position of box (integer)
3. ybox vertical position of box (integer)
4. width width of box (integer)
5. hight height of box (integer)
6. onpix total # on pixels (integer)
7. xbar mean x of on pixels in box (integer)
8. ybar mean y of on pixels in box (integer)
9. x2bar mean x variance (integer)
10. y2bar mean y variance (integer)
11. xybar mean x y correlation (integer)
12. x2ybar mean of x * x * y (integer)
13. xy2bar mean of x * y * y (integer)
14. xedge mean edge count left to right (integer)
15. xedgey correlation of xedge with y (integer)
16. yedge mean edge count bottom to top (integer)
17. yedgex correlation of yedge with x (integer)
"""

#first 5 row from data
letters.head()

#last 5 row from data
letters.tail()

#about the dataset

#dimensions
print("Dimensions:-","\n\n","(Rows,Columns)","\n", letters.shape, "\n\n\n")

#data types
print(letters.info())

#extract latters from data and sorted
Order =list(np.sort(letters['letter'].unique()))
print(Order)

# a quirky bug: the column names have a space, e.g. 'xbox ', which throws and error when indexed
print(letters.columns)

# let's 'reindex' the column names
letters.columns = ['letter', 'xbox', 'ybox', 'width', 'height', 'onpix', 'xbar',
       'ybar', 'x2bar', 'y2bar', 'xybar', 'x2ybar', 'xy2bar', 'xedge',
       'xedgey', 'yedge', 'yedgex']
print(letters.columns)

# basic plots: How do various attributes vary with the letters

plt.figure(figsize=(16, 8))
sns.barplot(x='letter', y='xbox', 
            data=letters, 
            order=Order)

letter_means = letters.groupby('letter').mean()
letter_means.head()

# average feature values
round(letters.drop('letter', axis=1).mean(), 2)

# splitting into X and y
X = letters.drop("letter", axis = 1)
y = letters['letter']

# scaling the features
#Create Object from MinMaxScaler
s = MinMaxScaler()
#fit_transform for dataset
X_scaled = s.fit_transform(X)

# train test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, random_state = 101)

# linear model

model_linear = SVC(kernel='linear')
model_linear.fit(X_train, y_train)

# predict
y_pred_linear = model_linear.predict(X_test)
print ('y_prediction' , y_pred_linear) 

# accuracy
print("accuracy:", metrics.accuracy_score(y_true=y_test, y_pred=y_pred_linear), "\n")
# cm
print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_linear))

# non-linear model
# using rbf kernel, C=1, default value of gamma

# model
non_linear_model = SVC(kernel='rbf')

# fit
non_linear_model.fit(X_train, y_train)

# predict
y_pred_rbf = non_linear_model.predict(X_test)
print('y_prediction is :' ,y_pred_rbf )

# accuracy
print("accuracy:", metrics.accuracy_score(y_true=y_test, y_pred=y_pred_rbf), "\n")

# cm
print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_rbf))

# using sigmoid kernel, C=1, default value of gamma

# model
non_linear_model = SVC(kernel='sigmoid')
# fit
non_linear_model.fit(X_train, y_train)

# predict
y_pred_sigmoid = non_linear_model.predict(X_test)
print('y_prediction is ::' , y_pred_sigmoid)

# accuracy
print("accuracy :", metrics.accuracy_score(y_true=y_test, y_pred=y_pred_sigmoid), "\n")

# cm
print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_sigmoid))

# using poly kernel, C=1, default value of gamma

# model
non_linear_model = SVC(kernel='poly')
# fit
non_linear_model.fit(X_train, y_train)

# predict
y_pred_poly = non_linear_model.predict(X_test)
print('y_prediction is ::' , y_pred_poly)

# accuracy
print("accuracy :", metrics.accuracy_score(y_true=y_test, y_pred=y_pred_poly), "\n")

# cm
print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_poly))

print("accuracy with linear kernel :", metrics.accuracy_score(y_true=y_test, y_pred=y_pred_linear)*100,"\n")
print("accuracy with rbf kernel :",    metrics.accuracy_score(y_true=y_test, y_pred=y_pred_rbf)*100, "\n")
print("accuracy with sigmoid kernel :", metrics.accuracy_score(y_true=y_test, y_pred=y_pred_sigmoid)*100, "\n")
print("accuracy with poly kernel :",   metrics.accuracy_score(y_true=y_test, y_pred=y_pred_poly)*100, "\n")

"""
#From this comparison Poly-Kernel has highest accuracy of 94.46%

Thanks! 


"""

